{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IHgmxWG_7lnE"
   },
   "source": [
    "# Введение в анализ данных\n",
    "## НИУ ВШЭ, 2019-2020 учебный год\n",
    "\n",
    "### Домашнее задание №3\n",
    "\n",
    "Задание выполнил(а): Тибилов Таймруаз\n",
    "\n",
    "### Общая информация\n",
    "\n",
    "__Дата выдачи:__ 08.04.2020\n",
    "\n",
    "__Дедлайн:__ 23:59 22.04.2020\n",
    "\n",
    "\n",
    "\n",
    "__Внимание!__ Домашнее задание выполняется самостоятельно. «Похожие» решения считаются плагиатом и все задействованные студенты (в том числе те, у кого списали) не могут получить за него больше 0 баллов.\n",
    "\n",
    "### Формат сдачи\n",
    "\n",
    "Загрузка файлов с решениями происходит в системе [Anytask](https://anytask.org/).\n",
    "\n",
    "\n",
    "Перед отправкой перезагрузите ноутбук и проверьте, что все ячейки могут быть последовательно выполнены. Ноутбук должен запускаться с использованием python 3.6+"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztx03xvr9T95"
   },
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BVrrwTJNjuDt"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XmgOCF3oqTnx",
    "outputId": "5cec33ad-528d-4eb5-e0cd-db45ab86a151"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:10<00:00,  9.89it/s]\n"
     ]
    }
   ],
   "source": [
    "# чтобы видеть проход по итерациям, можно использовать библиотеку tqdm\n",
    "# она работает примерно так:\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "    time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rvXKae8q9nn-"
   },
   "source": [
    "### Данные\n",
    "\n",
    "Мы имеем дело с данными с торговой платформы Avito.\n",
    "Для каждого товара представлены следующие параметры:\n",
    " - `'title'`\n",
    " - `'description'`\n",
    " - `'Category_name'`\n",
    " - `'Category'`\n",
    "\n",
    "Имеется информация об объектах 50 классов.\n",
    "Задача: по новым объектам (`'title'`, `'description'`) предсказать `'Category'`.\n",
    "(Очевидно, что параметр `'Category_name'` для предсказания классов использовать нельзя)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "BqEuoDhqNgoa",
    "outputId": "77b5fb87-b01c-4cc6-9369-caa921f511b4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>Category_name</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>382220</td>\n",
       "      <td>Прихожая</td>\n",
       "      <td>В хорошем состоянии. Торг</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>397529</td>\n",
       "      <td>Кордиант 215/55/16 Летние</td>\n",
       "      <td>Кордиант 215/55/16 Летние/\\n /\\nАртикул: 1737l...</td>\n",
       "      <td>Запчасти и аксессуары</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>584569</td>\n",
       "      <td>Стол</td>\n",
       "      <td>Стол, 2 рабочих места . Стол серого цвета, в д...</td>\n",
       "      <td>Мебель и интерьер</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2513100</td>\n",
       "      <td>Комбинезон</td>\n",
       "      <td>Размер-42/44</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1091886</td>\n",
       "      <td>Ветровка</td>\n",
       "      <td>На 2 года</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                      title  ...              Category_name Category\n",
       "0   382220                   Прихожая  ...          Мебель и интерьер       20\n",
       "1   397529  Кордиант 215/55/16 Летние  ...      Запчасти и аксессуары       10\n",
       "2   584569                       Стол  ...          Мебель и интерьер       20\n",
       "3  2513100                 Комбинезон  ...  Одежда, обувь, аксессуары       27\n",
       "4  1091886                   Ветровка  ...     Детская одежда и обувь       29\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"https://raw.githubusercontent.com/blacKitten13/minor2020-iad4/master/hw3/avito_data.csv\")\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Kg8iPp7fiwGh"
   },
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1hvzAMETU2d"
   },
   "outputs": [],
   "source": [
    "X = data[['title', 'description']].to_numpy()\n",
    "y = data['Category'].to_numpy()\n",
    "\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tMYU7zZw_cw-"
   },
   "source": [
    "Сразу разделим выборку на train и test.\n",
    "Никакие данные из test для обучения использовать нельзя!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6fia4_3vNprp"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "qDR8LtTJUIGt",
    "outputId": "dcf0bfcf-4bb1-4254-ba7f-6062cdfa0d17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Сапоги 46 размер новые', 'Сапоги 46 размер новые'],\n",
       "       ['Светильники потолочный swarovski',\n",
       "        'светильники потолочные swarovski 6 штук , цена за штуку. В эксплуатации 2 года , продаются в связи со сменой интерьера в квартире'],\n",
       "       ['iPhone 7 plus 128GB Red красный в наличии',\n",
       "        '\\xa0/\\n/\\n Данная цена только для подписчиков Instagram: iQmac/\\n/\\n Новый красный айфон 7 Plus в наличии это элегантный и мощный смартфон, который готов в полной мере раскрыть новые возможности iOS 10. Аппарат с 4-ядерным процессором А10 и 3 ГБ ОЗУ с легкостью решает самые ресурсоемкие задачи, позволяя наслаждаться быстродействием «тяжелых» приложений и игр на 5,5-дюймовом дисплее. Аппарат получил экран, как у iPad Pro, так что картинка теперь соответствует кинематографическому стандарту.'],\n",
       "       ['Пион Ирис Ромашка рассада',\n",
       "        'Пион куст 500 р ( более 10 шт)/\\nСаженец/ корень 100р/\\nРастут у нас более 70 лет/\\nРозовые, бордовые и белые/\\nНа фото цветы 2018г/\\nП. Зубчаниновка/\\nлибо пл. Революции/\\nЕсть ирисы, ромашка, клубника, боярышник и ирга'],\n",
       "       ['Кофта', 'Состояние отличное']], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "djycFF8mqToE"
   },
   "outputs": [],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-ZEdlEGAXTD"
   },
   "source": [
    "### Токенизация\n",
    "\n",
    "\n",
    "Токенизация -- разбиение текста на мелкие части, которые можно обработать машинными методами.\n",
    "Можно использовать разные алгоритмы токенизации. В данном задании мы будем использовать `WordPunctTokenizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "sgX6zNVtqToG",
    "outputId": "f5cbb010-d6f0-4b3f-e054-7e42cfe6454b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...\n",
      "after: ['здраствуйте', '.', 'я', ',', 'кирилл', '.', 'хотел', 'бы', 'чтобы', 'вы', 'сделали', 'игру', ',', '3д', '-', 'экшон', 'суть', 'такова', '...']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "text = 'Здраствуйте. Я, Кирилл. Хотел бы чтобы вы сделали игру, 3Д-экшон суть такова...'\n",
    "\n",
    "print(\"before:\", text,)\n",
    "print(\"after:\", tokenizer.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "x_RYBKC26o1X"
   },
   "source": [
    "__Задание:__ реализуйте функцию ниже."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O9VgNlZ1Qy3o"
   },
   "outputs": [],
   "source": [
    "def preprocess(text: str, tokenizer=WordPunctTokenizer()) -> str:\n",
    "    \"\"\"\n",
    "    Данная функция принимает на вход текст, \n",
    "    а возвращает тот же текст, но с пробелами между каждым токеном\n",
    "    \"\"\"\n",
    "    return ' '.join(tokenizer.tokenize(text.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6HmWNorpqToN"
   },
   "outputs": [],
   "source": [
    "assert preprocess(text, tokenizer) == 'здраствуйте . я , кирилл . хотел бы чтобы вы сделали игру , 3д - экшон суть такова ...'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RW9fTaFgqToQ"
   },
   "source": [
    "__Задание:__ токенизируйте `'title'` и `'description'` в `train` и `test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z5WO-7tJUvbs"
   },
   "outputs": [],
   "source": [
    "for i in range(X_train.shape[0]):\n",
    "    X_train[i, 0] = preprocess(X_train[i, 0])\n",
    "    X_train[i, 1] = preprocess(X_train[i, 1], tokenizer)\n",
    "\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_test[i, 0] = preprocess(X_test[i, 0], tokenizer)\n",
    "    X_test[i, 1] = preprocess(X_test[i, 1], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VDnDSWwFDwFo"
   },
   "outputs": [],
   "source": [
    "assert X_train[5][0] == '1 - к квартира , 33 м² , 4 / 5 эт .'\n",
    "assert X_train[10][1] == 'продам иж планета 3 , 76 год , ( стоит на старом учёте , документы утеряны ) на ходу , хорошее состояние , все интересующие вопросы по телефону ( с родной коляской на 3 тысячи дороже ) . торга не будет .'\n",
    "assert X_test[2][0] == 'фара правая toyota rav 4 галоген 2015 - 19'\n",
    "assert X_test[2][1] == 'фара правая для toyota rav4 2015 / оригинальный номер : 8113042650 / тойота рав4 тоета рав 4 / производитель : toyota / состояние : отличное без дефектов ! / комментарий : после 2015 не ксенон галоген + диод / пожалуйста , уточняйте соответствие вашего заказа изображенному на фото . / звоните уточняйте по наличию предоставляется время на проверку детали / отправляем в регионы рф транспортными компаниями / . / всегда включен вайбер вацап по вопросам !/ дополнительное фото по запросу'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hlIITUk0AsmS"
   },
   "source": [
    "### BOW\n",
    "\n",
    "Один из традиционных подходов -- построение bag of words.\n",
    "\n",
    "Метод состоит в следующем:\n",
    "\n",
    " - Составить словарь самых часто встречающихся слов в `train data`\n",
    " - Для каждого примера из `train` посчитать, сколько раз каждое слово из словаря в нём встречается\n",
    "\n",
    "\n",
    " В `sklearn` есть `CountVectorizer`, но в этом задании его использовать нельзя."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GMKUttDWIF92"
   },
   "source": [
    "__Задание:__ создайте словарь, где каждому токену соответствует количество раз, которое оно встретилось в `X_train`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bailNzS1qToV"
   },
   "outputs": [],
   "source": [
    "tokens_cnt = {}\n",
    "\n",
    "for i in range(X_train.shape[0]):\n",
    "    for j in tokenizer.tokenize(X_train[i, 0]):\n",
    "        if j not in tokens_cnt.keys():\n",
    "            tokens_cnt[j] = 0\n",
    "        tokens_cnt[j] += 1;\n",
    "    for j in tokenizer.tokenize(X_train[i, 1]):\n",
    "        if j not in tokens_cnt.keys():\n",
    "            tokens_cnt[j] = 0\n",
    "        tokens_cnt[j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o2qym8MaqToX"
   },
   "outputs": [],
   "source": [
    "assert tokens_cnt['сапоги'] == 454"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rqjPjjA9qToa"
   },
   "source": [
    "__Задание:__ выведите 10 самых частотных и 10 самых редких токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "AgdD7XNmqToa",
    "outputId": "49644f38-9b2e-402a-f66a-59ebf2e5666f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 10: \n",
      "/ , . - в и на ./ : с\n",
      "Последние 10: \n",
      "шуршат гремят петровского столиц объективную понравившейся беспрецедентно дооснастить хлебозаводская фрионом\n"
     ]
    }
   ],
   "source": [
    "sorted_tokens = sorted(tokens_cnt, key=lambda x: tokens_cnt[x], reverse=True)\n",
    "print('Первые 10: ')\n",
    "print(*sorted_tokens[:10])\n",
    "print('Последние 10: ')\n",
    "print(*sorted_tokens[len(sorted_tokens) - 10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cHqfVzwJqTod"
   },
   "source": [
    "__Задание:__ оставьте в словаре только топ-10000 самых частотных токенов, также создайте отдельный список из этих слов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gj5rPE9-qToe"
   },
   "outputs": [],
   "source": [
    "for item in sorted_tokens[10000:]:\n",
    "    del tokens_cnt[item]\n",
    "tokens_list = sorted_tokens[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "585NkUdvqTog"
   },
   "source": [
    "__Задание:__ реализуйте функцию, которая переводит текст в вектор из чисел. То есть каждому токену из списка токенов сопоставляется количество раз, которое он встретился в тексте."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4awkhecbR9om"
   },
   "outputs": [],
   "source": [
    "def text_to_bow(text: str, tokens_list: list) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из словаря\n",
    "    указано количество его употреблений в предложении\n",
    "    input: строка, список токенов\n",
    "    output: вектор той же размерности, что и список токенов\n",
    "    \"\"\"\n",
    "    \n",
    "    result = np.zeros(10000)\n",
    "    for token in text.split():\n",
    "        if token in tokens_list:\n",
    "            result[tokens_list.index(token)] += 1\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pri5t6NzqToi"
   },
   "outputs": [],
   "source": [
    "example_text = text_to_bow(\"сдаётся уютный , тёплый гараж для стартапов в ml\", tokens_list)\n",
    "\n",
    "assert np.allclose(example_text.mean(), 0.0008)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Inf8BKbvqTok"
   },
   "source": [
    "__Задание:__ а теперь реализуйте функцию, которая преобразует наш датасет и каждому тексту из `'description'` сопоставляет вектор."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HR_D8Fn4pudv"
   },
   "outputs": [],
   "source": [
    "def descr_to_bow(items: np.array, tokens_list: list, col=1) -> np.array:\n",
    "    \"\"\" Для каждого описания товара возвращает вектор его bow \"\"\"\n",
    "    \n",
    "    bow_list = []\n",
    "    for item in tqdm(items):\n",
    "        bow_list.append(text_to_bow(item[col], tokens_list))\n",
    "    return np.array(bow_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "wwOZaEpMSQsZ",
    "outputId": "47b7cbda-94b4-4bb2-b508-5350a86b6a63"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [01:06<00:00, 317.67it/s]\n",
      "100%|██████████| 9000/9000 [00:29<00:00, 309.66it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_bow = descr_to_bow(X_train, tokens_list)\n",
    "X_test_bow = descr_to_bow(X_test, tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2R5JGvB-qToo"
   },
   "outputs": [],
   "source": [
    "assert X_train_bow.shape == (21000, 10000), X_test_bow.shape == (9000, 10000)\n",
    "assert 0.005 < X_train_bow.mean() < 0.006\n",
    "assert 0.005 < X_test_bow.mean() < 0.006"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vJoXiCWI7VF5"
   },
   "source": [
    "### Логистическая регрессия и SVC\n",
    "\n",
    "\n",
    "Теперь описание каждого товара представлено, как точка в многомерном пространстве.\n",
    "Очень важно запомнить эту идею: дальше мы будем рассматривать разные способы перехода от текста к точке в пространстве.\n",
    "\n",
    "Для BOW каждое измерение в пространстве -- какое-то слово.\n",
    "Мы предполагаем, что текст описывается набором каких-то популярных слов, которые в нём встречаются, а близкие по смыслу тексты будут использовать одинаковые слова.\n",
    "\n",
    "Обучите логистическую регрессию и SVM с линейным ядром (`sklearn.svm.LinearSVC` или `sklearn.svm.SVC(kernel='linear')`) с базовыми параметрами. При необходимости можете увеличить максимальное число итераций. В качестве `random_state` возьмите 13."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gz6L7NvHqTor"
   },
   "source": [
    "_Подсказка: для того, чтобы было проще обучать, можно использовать [разреженные матрицы](https://ru.wikipedia.org/wiki/%D0%A0%D0%B0%D0%B7%D1%80%D0%B5%D0%B6%D0%B5%D0%BD%D0%BD%D0%B0%D1%8F_%D0%BC%D0%B0%D1%82%D1%80%D0%B8%D1%86%D0%B0) - многие модели из `sklearn` умеют с ними работать. Соответствующий модуль из `scipy`: [scipy.sparse](https://docs.scipy.org/doc/scipy/reference/sparse.html). Нетрудно заметить, что в полученных BOW-матрицах очень много нулей. Если хранить в памяти только ненулевые элементы, можно сильно оптимизировать вычисления. Можете в этом убедиться:_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "aQUTSUnBqTor",
    "outputId": "f02fa24e-f009-4e0e-a90e-7d547659a24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train array in memory (raw): 1680.000 Mb\n",
      "Train array in memory (compressed): 8.606 Mb\n"
     ]
    }
   ],
   "source": [
    "print('Train array in memory (raw): {:.3f} Mb'.format(X_train_bow.nbytes * 1e-6))\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "X_train_bow_csr = csr_matrix(X_train_bow)\n",
    "print('Train array in memory (compressed): {:.3f} Mb'.format(\n",
    "    (X_train_bow_csr.data.nbytes + X_train_bow_csr.indptr.nbytes + X_train_bow_csr.indices.nbytes) * 1e-6)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJVLS8Fs3CeT"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oapQNZ20Wr6g"
   },
   "outputs": [],
   "source": [
    "X_test_bow_csr = csr_matrix(X_test_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ky3HV1rTSS9L"
   },
   "outputs": [],
   "source": [
    "logit = LogisticRegression(random_state=13, max_iter=5000)\n",
    "logit.fit(X_train_bow_csr, y_train)\n",
    "y_pred = logit.predict(X_test_bow_csr)\n",
    "\n",
    "assert accuracy_score(y_test, y_pred) > 0.695"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "SzNa76-nnFeI",
    "outputId": "bdc502e2-3823-4f00-e625-353a6dcec839"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "svm = LinearSVC(random_state=13, max_iter=5000)\n",
    "svm.fit(X_train_bow_csr, y_train)\n",
    "y_pred = svm.predict(X_test_bow_csr)\n",
    "\n",
    "assert accuracy_score(y_test, y_pred) > 0.68"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WwKE57YZ1Hzn"
   },
   "source": [
    "### Модификация признаков"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ewMlxQezL6Ax"
   },
   "source": [
    "Прибавьте к соответствующим BOW-векторам BOW-вектора для `'title'` товара с некоторым весом. Изменится ли качество? Как вы можете это объяснить?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "5tTRcyc0qTo0",
    "outputId": "50e04ea9-4591-4907-b139-e5ac8cdb7b0a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [00:07<00:00, 2857.95it/s]\n",
      "100%|██████████| 9000/9000 [00:03<00:00, 2877.68it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_bow_full = X_train_bow + 1.9 * descr_to_bow(X_train, tokens_list, col=0)\n",
    "X_test_bow_full = X_test_bow + 1.9 * descr_to_bow(X_test, tokens_list, col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pESVL_s-4xBI"
   },
   "outputs": [],
   "source": [
    "X_train_bow_full_csr = csr_matrix(X_train_bow_full)\n",
    "X_test_bow_full_csr = csr_matrix(X_test_bow_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "V8Kzj0WX5Fmd",
    "outputId": "9531ef00-dae8-4f80-dca1-3b518d16bdbf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 0.6841111111111111, 0.7608888888888888)"
      ]
     },
     "execution_count": 34,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm = LinearSVC(random_state=13, max_iter=5000)\n",
    "svm.fit(X_train_bow_full_csr, y_train)\n",
    "y_new_pred = svm.predict(X_test_bow_full_csr)\n",
    "\n",
    "accuracy_score(y_test, y_pred) < accuracy_score(y_test, y_new_pred), accuracy_score(y_test, y_pred), accuracy_score(y_test, y_new_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ulnmBlch50sE"
   },
   "source": [
    "__Пояснение:__ В данном случае обучение происходит гораздо лучше, так как `title` в основном содержит ключевые слова, по которым можно точнее определить категорию товара. Например: платье, сапоги, кофта - эти слова можно встретить в основном в названиях товаров категории \"Одежда\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Db4TyqzxMnby"
   },
   "source": [
    "Нормализуйте данные с помощью `MinMaxScaler` или `MinAbsScaler` перед обучением. Что станет с качеством и почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A8rVy6q1Mn4J",
    "outputId": "6008e92d-1f89-4901-d37f-759b7993286f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, 0.7608888888888888, 0.7723333333333333)"
      ]
     },
     "execution_count": 35,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_bow_full = scaler.fit(X_train_bow_full).transform(X_train_bow_full)\n",
    "X_test_bow_full = scaler.fit(X_test_bow_full).transform(X_test_bow_full)\n",
    "\n",
    "X_train_bow_full_csr = csr_matrix(X_train_bow_full)\n",
    "X_test_bow_full_csr = csr_matrix(X_test_bow_full)\n",
    "y_pred = y_new_pred\n",
    "\n",
    "csv = LinearSVC(random_state=13, max_iter=5000)\n",
    "csv.fit(X_train_bow_full_csr, y_train)\n",
    "y_new_pred = csv.predict(X_test_bow_full_csr)\n",
    "\n",
    "accuracy_score(y_test, y_pred) < accuracy_score(y_test, y_new_pred), accuracy_score(y_test, y_pred), accuracy_score(y_test, y_new_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aysOqz8-9IrJ"
   },
   "source": [
    "__Пояснение:__ В данном случае после нормализации можно наблюдать незначительное улучшение результата обучения. Большого прироста это не дает, так как все признаки имеют одно и то же измерение и, по сути, уже являются нормированными относительно своих значений."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-dow-XtaqTo5"
   },
   "source": [
    "Почему в данном случае использовать `StandardScaler` - не очень хорошая идея?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wXatf5U89uKb"
   },
   "source": [
    "__Ответ:__ Если мы будем использовать `StandardScaler`, то вместо нулевых значений (коих в наших векторах полно) мы получим ненулевые отрицательные значения, что очень плохо скажется на работе нашей модели, так как в необработанном состоянии по сути признаки сами зануляли себя. После же такой обработки модель получится неустойчивой, а разброс такой модели увеличится."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvCAL3qGDByj"
   },
   "source": [
    "### Иная предобработка\n",
    "\n",
    "**На выбор**:\n",
    "\n",
    "- **либо** обучите модели, используя для предобработки токенизатор и лемматизатор `pymystem3.Mystem`.\n",
    "- **либо** добавьте к предобработке стэмминг.\n",
    "\n",
    "Сравните полученное сейчас качество с полученным ранее и сделайте вывод."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NAtZB4_KFhLs"
   },
   "source": [
    "__Пояснение:__ Далее будет представлено решение с другим стеммером, так как стеммер-лемматизатор от Яндекса у меня выдает ошибку, не связанную с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3iSR28vI--X"
   },
   "outputs": [],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "stem = SnowballStemmer('russian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mGvNHfVsDfhq"
   },
   "outputs": [],
   "source": [
    "stem_tokens_cnt = {}\n",
    "\n",
    "for i in tqdm(range(X_train.shape[0])):\n",
    "    for j in tokenizer.tokenize(X_train[i, 0]):\n",
    "        j = stem.stem(j)\n",
    "        if j not in stem_tokens_cnt.keys():\n",
    "            stem_tokens_cnt[j] = 0\n",
    "        stem_tokens_cnt[j] += 1;\n",
    "    for j in tokenizer.tokenize(X_train[i, 1]):\n",
    "        j = stem.stem(j)\n",
    "        if j not in stem_tokens_cnt.keys():\n",
    "            stem_tokens_cnt[j] = 0\n",
    "        stem_tokens_cnt[j] += 1\n",
    "\n",
    "stem_tokens_cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "LoicLGVhRyYZ",
    "outputId": "a5bf9755-a7a1-4a1c-c091-bd6896e954f7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Первые 10: \n",
      "/ , . - в и на ./ : с\n",
      "Последние 10: \n",
      "25j220 x700 икроножн toddler basics шуршат объективн дооснаст хлебозаводск фрион\n"
     ]
    }
   ],
   "source": [
    "stem_sorted_tokens = sorted(stem_tokens_cnt, key=lambda x: stem_tokens_cnt[x], reverse=True)\n",
    "print('Первые 10: ')\n",
    "print(*stem_sorted_tokens[:10])\n",
    "print('Последние 10: ')\n",
    "print(*stem_sorted_tokens[len(stem_sorted_tokens) - 10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ekmi18GXSEfd"
   },
   "outputs": [],
   "source": [
    "for item in stem_sorted_tokens[10000:]:\n",
    "    del stem_tokens_cnt[item]\n",
    "stem_tokens_list = stem_sorted_tokens[:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 292
    },
    "colab_type": "code",
    "id": "JsII75MpSYNN",
    "outputId": "518465f4-6380-468c-814f-f0369ab693c4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [01:13<00:00, 287.56it/s]\n",
      "100%|██████████| 9000/9000 [00:31<00:00, 285.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([['винт сдвижн панел потолк бмв х5 е53',\n",
       "        '\"* 3308 * винт сдвижн панел потолк o : 54107199476 ( 54 10 7 199 476 ) / / запчаст б / у в хорош состоян для bmw x5 e53 , / демонтирова с автомобил из япон без пробег по рф . гарант на все запчаст 14 дне . возможн зам / установк в автосервис партнер . / примечан : / / для упрощен поиск запчаст сообщ нам внутрен код : * 3308 * / в налич на склад : 16 шт . / проверя соответств дан запчаст по оригинальн номер о - ваш автомобил по vin . бол 1000 наименован запасн част для bmw на наш склад в москв . отправля по все росс почт ил транспортн компан \"'],\n",
       "       ['сандалик tott для мальчик',\n",
       "        'прода сандалик tott для малыш ! размер 20 , по стельк 12 см .'],\n",
       "       ['фар прав toyot rav 4 галог 2015 - 19',\n",
       "        'фар прав для toyot rav4 2015 / оригинальн номер : 8113042650 / тойот рав4 тоет рав 4 / производител : toyot / состоян : отличн без дефект ! / комментар : посл 2015 не ксенон галог + диод / пожалуйст , уточня соответств ваш заказ изображен на фот . / звон уточня по налич предоставля врем на проверк дета / отправля в регион рф транспортн компан / . / всегд включ вайбер вацап по вопрос !/ дополнительн фот по запрос'],\n",
       "       ...,\n",
       "       ['светодиодн профильн прожектор 300вт 3200к',\n",
       "        'светодиодн профильн прожектор 300вт 3200к yuesheng ys - 300z - w ./ нов . в налич 2шт ./ светодиодн профилир прожектор бел свет . цветов температур 3200к . ручн зум - 17 °- 50 °./ управлен : dmx 512 ( 3 xlr ) lcd диспл / реж : мастер / слэйв / кол - во канал dmx : 3 / источник свет : 300w whit cob led , 3200k / cri : ra > 85led / габарит : 62 * 29 * 22 см / вес : 10кг / напряжен питан : 100 - 240в / 50гц / потребля мощност : 320вт ( макс )'],\n",
       "       ['плат праздничн', 'великолепн праздничн плат в отличн состоян !'],\n",
       "       ['прод производствен помещен , 90 м²',\n",
       "        'продаж встроен нежил помещен ряд с м . чкаловск на ул . больш зеленин . удален от метр 250 м , вход со сторон улиц со двор , внутр котор можн паркова .. помещен площад 90 кв . м располож в подвал жил дом , состо из двух зал и входн групп , сдела ремонт , высот потолк 2 , 31 м , заглублен 1 , 3 м . объект хорош подход для организац магазин , сфер услуг . клиент легк будет найт проход к помещен и легк добра от метр . в настоя врем сдан в аренд под магазин костюм . продаж с арендатор .']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_stem = np.array(X_train)\n",
    "X_test_stem = np.array(X_test)\n",
    "\n",
    "for i in tqdm(range(X_train.shape[0])):\n",
    "    X_train_stem[i, 0] = (' '.join([stem.stem(w) for w in tokenizer.tokenize(X_train[i, 0])]))\n",
    "    X_train_stem[i, 1] = (' '.join([stem.stem(w) for w in tokenizer.tokenize(X_train[i, 1])]))\n",
    "\n",
    "for i in tqdm(range(X_test.shape[0])):\n",
    "    X_test_stem[i, 0] = (' '.join([stem.stem(w) for w in tokenizer.tokenize(X_test[i, 0])]))\n",
    "    X_test_stem[i, 1] = (' '.join([stem.stem(w) for w in tokenizer.tokenize(X_test[i, 1])]))\n",
    "\n",
    "X_test_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "Y1uDiNmjVZMV",
    "outputId": "72064399-d4a9-4964-80bd-a46d1c155f74"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [00:45<00:00, 458.19it/s]\n",
      "100%|██████████| 21000/21000 [00:06<00:00, 3326.70it/s]\n",
      "100%|██████████| 9000/9000 [00:20<00:00, 442.87it/s]\n",
      "100%|██████████| 9000/9000 [00:02<00:00, 3375.01it/s]\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(True, 0.7732222222222223, 0.7806666666666666)"
      ]
     },
     "execution_count": 42,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_stem_bow = descr_to_bow(X_train_stem, stem_tokens_list) + 1.8 * descr_to_bow(X_train_stem, stem_tokens_list, col=0)\n",
    "X_test_stem_bow = descr_to_bow(X_test_stem, stem_tokens_list) + 1.8 * descr_to_bow(X_test_stem, stem_tokens_list, col=0)\n",
    "\n",
    "X_train_stem_bow_csr = csr_matrix(X_train_stem_bow)\n",
    "X_test_stem_bow_csr = csr_matrix(X_test_stem_bow)\n",
    "\n",
    "svc = LinearSVC(random_state=13, max_iter=5000)\n",
    "svc.fit(X_train_stem_bow_csr, y_train)\n",
    "y_new_pred = svc.predict(X_test_stem_bow_csr)\n",
    "\n",
    "accuracy_score(y_test, y_pred) < accuracy_score(y_test, y_new_pred), accuracy_score(y_test, y_pred), accuracy_score(y_test, y_new_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5Sk60yojXHgC"
   },
   "source": [
    "__Пояснение:__ После стемминга точность предсказания повысилась\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bXbsPtpfoB7m"
   },
   "source": [
    "### TF-IDF\n",
    "\n",
    "Не все слова полезны одинаково, давайте попробуем [взвесить](http://tfidf.com/) их, чтобы отобрать более полезные.\n",
    "\n",
    "\n",
    "> TF(t) = (Number of times term t appears in a document) / (Total number of terms in the document).\n",
    "> \n",
    "> IDF(t) = log_e(Total number of documents / Number of documents with term t in it).\n",
    "\n",
    "\n",
    "В `sklearn` есть `TfidfVectorizer`, но в этом задании его использовать нельзя. Для простоты посчитайте общий tf-idf для `'title'` и `'description'` (то есть каждому объекту надо сопоставить вектор, где как документ будет рассматриваться конкатенация `'title'` и `'description'`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sKu0o5-FqTo8"
   },
   "source": [
    "__Задание:__ составьте словарь, где каждому слову из изначального списка будет соответствовать количество документов из `train`-части, где это слово встретилось."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bvKizoKUqTo8",
    "outputId": "331a39c7-00ec-40f7-a1d2-25a7efc833de"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [00:00<00:00, 25554.14it/s]\n"
     ]
    }
   ],
   "source": [
    "word_document_cnt = {}\n",
    "for i in tqdm(X_train):\n",
    "    curr = set()\n",
    "    for j in i[0].split():\n",
    "        curr.add(j)\n",
    "    for j in i[1].split():\n",
    "        curr.add(j)\n",
    "    for j in curr:\n",
    "        if j not in word_document_cnt.keys():\n",
    "            word_document_cnt[j] = 0\n",
    "        word_document_cnt[j] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mscUMtzLqTpB"
   },
   "outputs": [],
   "source": [
    "assert word_document_cnt['размер'] == 2839"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0XGwjqOeqTpD"
   },
   "source": [
    "__Задание:__ реализуйте функцию, где тексту в соответствие ставится tf-idf вектор. Для вычисления IDF также необходимо число документов в `train`-части (параметр `n_documents_total`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6i5zFpD9rbtz"
   },
   "outputs": [],
   "source": [
    "def text_to_tfidf(text: str, word_document_cnt: dict, tokens_list: list, n_documents_total: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает вектор, где для каждого слова из словаря\n",
    "    указан tf-idf\n",
    "    \"\"\"\n",
    "    \n",
    "    result = np.zeros(len(tokens_list))\n",
    "    curr = text.split()\n",
    "    for w in curr:\n",
    "        if w in tokens_list:\n",
    "            result[tokens_list.index(w)] = curr.count(w) / float(len(curr)) * np.log(float(n_documents_total) / word_document_cnt[w])\n",
    "    return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrjJ9iFhqTpG"
   },
   "outputs": [],
   "source": [
    "example_text = text_to_tfidf(\n",
    "    'сдаётся уютный , тёплый гараж для стартапов в ml',\n",
    "    word_document_cnt,\n",
    "    tokens_list,\n",
    "    n_documents_total=len(X_train)\n",
    ")\n",
    "assert 0.0003 < example_text.mean() < 0.0004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2fXFolhBqTpI"
   },
   "source": [
    "__Задание:__ а теперь реализуйте функцию, которая преобразует наш датасет и для каждого объекта сопоставляет вектор tf-idf. В качестве текстов используйте конкатенацию `'title'` и `'description'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fi_A3Ea3qTpJ"
   },
   "outputs": [],
   "source": [
    "def items_to_tfidf(items: np.array, word_document_cnt: dict, tokens_list: list, n_documents_total: int) -> np.array:\n",
    "    \"\"\"\n",
    "    Для каждого товара возвращает его tf-idf вектор\n",
    "    \"\"\"\n",
    "    \n",
    "    res = []\n",
    "    for i in tqdm(items):\n",
    "        res.append(text_to_tfidf(i[0] + ' ' + i[1], word_document_cnt, tokens_list, n_documents_total))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s-niY8cJqTpL"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf = items_to_tfidf(X_train, word_document_cnt, tokens_list, len(X_train))\n",
    "X_test_tfidf = items_to_tfidf(X_test, word_document_cnt, tokens_list, len(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSBFAmRrqTpO"
   },
   "outputs": [],
   "source": [
    "assert X_train_tfidf.shape == (21000, 10000), X_test_tfidf.shape == (9000, 10000)\n",
    "assert 0.0002 < X_train_tfidf.mean() < 0.0004\n",
    "assert 0.0002 < X_test_tfidf.mean() < 0.0004"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-YFA-8kE1RHk"
   },
   "source": [
    "__Задание:__ обучите логистическую регрессию и SVC, оцените качество (accuracy_score). Сделайте вывод."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Xr7i0jA4kJSB"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mp8eNMsQnEbV"
   },
   "outputs": [],
   "source": [
    "X_train_tfidf_csr = csr_matrix(X_train_tfidf)\n",
    "X_test_tfidf_csr = csr_matrix(X_test_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-ULrXsF1m5sU"
   },
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(random_state=13, max_iter=5000)\n",
    "lr_model.fit(X_train_tfidf_csr, y_train)\n",
    "\n",
    "assert accuracy_score(y_test, lr_model.predict(X_test_tfidf_csr)) > 0.675"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XUUF9c-oqTpS"
   },
   "outputs": [],
   "source": [
    "svc_model = LinearSVC(random_state=13, max_iter=5000)\n",
    "svc_model.fit(X_train_tfidf_csr, y_train)\n",
    "\n",
    "assert accuracy_score(y_test, svc_model.predict(X_test_tfidf_csr)) > 0.79"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vQZ61xSsTpZI"
   },
   "source": [
    "### Word Vectors\n",
    "\n",
    "Давайте попробуем другой подход -- каждому слову сопоставим какое-то векторное представление (эмбеддинг) - но достаточно маленькой размерности. Таким образом мы сильно уменьшим количество параметров в модели.\n",
    "\n",
    "Почитать про это подробнее можно тут:\n",
    "\n",
    "- https://habr.com/ru/company/ods/blog/329410/\n",
    "\n",
    "Вектора мы возьмём уже готовые (обученные на текстах из интернета), так что наша модель будет знать некоторую дополнительную информацию о внешнем мире."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T38J27NcYGx5"
   },
   "outputs": [],
   "source": [
    "!wget https://www.dropbox.com/s/0x7oxso6x93efzj/ru.tar.gz\n",
    "# если не работает (возможно, у вас windows) - можете скачать файл по соответствующей ссылке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zfse4xVbgMIr"
   },
   "outputs": [],
   "source": [
    "!tar -xzf ru.tar.gz\n",
    "# распаковка файла - опять же, если не работает, распакуйте вручную"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tl2jtBu0qTpZ"
   },
   "outputs": [],
   "source": [
    "!pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sy2TXmQ2jZSY"
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models.wrappers import FastText\n",
    "\n",
    "embedding_model = FastText.load_fasttext_format('ru.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6LCYFr9LqTpe"
   },
   "outputs": [],
   "source": [
    "# как мы видим, каждому слову данная модель сопоставляет вектор размерности 300\n",
    "\n",
    "print(embedding_model['привет'].shape)\n",
    "print(embedding_model['привет'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvGcZVr5x2P0"
   },
   "outputs": [],
   "source": [
    "embedding_model.wv.vocab.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ohZy6SXFqTpg"
   },
   "source": [
    "__Задание:__ реализуйте функцию, выдающую эмбеддинг для предложения - как сумму эмбеддингов токенов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H49QR_jhjmCa"
   },
   "outputs": [],
   "source": [
    "def sentence_embedding(sentence: str, embedding_model) -> np.array:\n",
    "    \"\"\"\n",
    "    Складывает вектора токенов строки sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    res = np.zeros(300)\n",
    "    for word in sentence.split():\n",
    "        try:\n",
    "            res = res + np.array(embedding_model[word])\n",
    "        except:\n",
    "            pass\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gj6U_hjtlllV"
   },
   "outputs": [],
   "source": [
    "assert sentence_embedding('сдаётся уютный , тёплый гараж для стартапов в ml', embedding_model).shape == (300,)\n",
    "assert np.allclose(np.linalg.norm(sentence_embedding('сдаётся уютный , тёплый гараж для стартапов в ml', embedding_model)), 2.6764746)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLMG0kZ2qTpj"
   },
   "source": [
    "__Задание:__ сделайте все то же, что в предыдущих пунктах -- реализуйте функцию, которая преобразует данные, а затем обучите логистическую регрессию и SVM, оцените качество. Сделайте вывод, что работает лучше - модель, основанная на TF-IDF, или модель, обученная на предобученных эмбеддингах?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8tfhc-PFmGvu"
   },
   "outputs": [],
   "source": [
    "def data_embedding(data: np.array, embedding_model) -> np.array:\n",
    "    \"\"\"\n",
    "    Возвращает обработанные тексты в виде векторов\n",
    "    \"\"\"\n",
    "\n",
    "    res = []\n",
    "    for i in tqdm(data):\n",
    "        res.append(sentence_embedding(i[0] + ' ' + i[1], embedding_model))\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "G6FhEdkQ3aVZ",
    "outputId": "fbcf57f5-0c56-4450-d8f0-4e5b9be85975"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 21000/21000 [00:23<00:00, 889.57it/s]\n",
      "100%|██████████| 9000/9000 [00:10<00:00, 870.37it/s]\n"
     ]
    }
   ],
   "source": [
    "X_train_embedded = data_embedding(X_train, embedding_model)\n",
    "X_test_embedded = data_embedding(X_test, embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z10xcPbe30nN"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "o2qZ7ZD-32GZ",
    "outputId": "0bb41c0f-fdc2-4959-d6d2-158374cbefd5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.583\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(random_state=13, max_iter=5000)\n",
    "lr.fit(X_train_embedded, y_train)\n",
    "\n",
    "print(accuracy_score(y_test, lr.predict(X_test_embedded)))\n",
    "print(accuracy_score(y_test, lr.predict(X_test_embedded)) > accuracy_score(y_test, lr_model.predict(X_test_tfidf_csr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hWuCVYOO4C0n"
   },
   "outputs": [],
   "source": [
    "svc = LinearSVC(random_state=13, max_iter=5000)\n",
    "svc.fit(X_train_embedded, y_train)\n",
    "\n",
    "print(accuracy_score(y_test, svc.predict(X_test_embedded)))\n",
    "print(accuracy_score(y_test, svc.predict(X_test_embedded)) > accuracy_score(y_test, svc_model.predict(X_test_tfidf_csr)))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "hw3_texts.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
